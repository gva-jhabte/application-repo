"""
CallExploitDB

Retrieves CVE -> ExploitDB mappings from Mitre, and retrieves ExploitDB database from ExploitDB

Merges these two to produce a jsonl file keyed by both CVE and ExploitDBId, as there ia s 
many to many relationship between these two. This can then act as a source of raw CVE/Explloit
data for subsequent reports.

Note that the source data is not date dependent. All the sources are externally
maintained and no not have histories.
"""

import datetime, os, sys
from datetime import timedelta
sys.path.insert(1, os.path.join(sys.path[0], '..'))
sys.path.insert(1, os.path.join(sys.path[0], '../..'))
from va_auditor import Auditor
import va_dt_common.common as dt
try:
    import ujson as json
except ImportError:
    import json
import pandas as pd

from google.cloud import storage


CONFIG_FILE = 'config.yaml'
config = dt.read_config(CONFIG_FILE)

JOB_NAME = config.get('job_name')
SOURCE_URL_EXPLOITDB = config.get('source_URL_ExploitDB')
SOURCE_URL_CVE_EDB_XREF = config.get('source_URL_CVE_EDB_XRef')
TARGET_PROJECT = config.get('target_project')
TARGET_BUCKET = config.get('target_bucket')
TARGET_BLOB = config.get('target_blob')
BLOB_FOR_MITRE_CVE_EXPLOITS = config.get('blob_for_Mitre_CVE_Exploits')
BLOB_FOR_EXPLOITDB = config.get('blob_for_ExploitDB')

def main():

    # keep a record of key data items so we can log what we've done
    auditor = Auditor(JOB_NAME, r'../../config/va_auditor.yaml')
    auditor.commencement_time = datetime.datetime.today()

    # set up a temp file for saving to
    # set the auditor to automatically track the written records 
    temp_File = dt.temp_file(JOB_NAME, auditor)

    # Read in data from ExploitDB. Gives a dataframe with one row per Exploit Id
    ExploitDBDataRaw = pd.read_csv(SOURCE_URL_EXPLOITDB)
    ExploitDBData = ExploitDBDataRaw.fillna('') # deal with missing port numbers
    # Rename the id column to match the desired column name for later use
    ExploitDBData['ExploitId'] = ExploitDBData.pop('id')


    # Read in data from CVE-EDB Xrefs. Output a list of json lines mapping CVE to EDB Id. Note that these are 
    # many to many so keyed by both fields.
    edb = pd.read_html(SOURCE_URL_CVE_EDB_XREF)
    edb = edb[3] # Use the fourth table which maps EDB -> CVEs

    # Each row of this DF contains a single ExploitId and a string of one or more CVEs. 
    for i in range(len(edb)):
        
        if len(edb[0][i].split(':')) < 2:
            continue  # Line returned has invalid data. Do not attempt to process
            
        sploitNumber = edb[0][i].split(':')[1] # converts 'ExploitDB:<number>' into number as an int
        if not isinstance(sploitNumber, int):
            continue # Again, data returned is invalid, it doesn't have an integer ExploitDB Id
            
        # Check if the Mitre EDB Id actually exists on the EDB. If it doesn't then ignore this row.
        if sploitNumber in ExploitDBData['ExploitId'].values:

            # Split the CVEs into an array in their own right
            CVEs = dt.find_cves(str(edb[1][i]))

            # and get the ExploitDB row that will be used to enrich from for each CVE.
            #
            # .loc finds for field=value. The 'records' parameter ensures that only the records, not the 
            # index number are taken into the output. The final [0] is because the to_dict('records') generates a list
            # not a dict. 
            ExploitDBRowDict = ExploitDBData.loc[ExploitDBData['ExploitId']==sploitNumber].to_dict('records')[0]

            for cve in CVEs:
                # Generate output combining the ExploitDB row with the CVEId
                row = dict({'CVE': cve}, **ExploitDBRowDict)  
                temp_File.write_json_line(row)

    # finally write out the temp file to the bucket after incorporating the run_date
    preFormat = TARGET_BLOB.replace('%date', '%Y-%m-%d')
    destinationFile = datetime.datetime.today().strftime(preFormat) # always write this as at today as the source data is always as at today
    temp_File.save_to_bucket(TARGET_PROJECT, TARGET_BUCKET, destinationFile)

    # No need to explicitly remove the local file. temp_file class has a destructor that will do that.
    temp_File = None

    # Write out the two source data files so that any confirmation of validity from sources can use
    # the source data as it was at the time of creation.
    #
    # Mitre CVE -> EDB XRef table to temporary file...
    temp_File = dt.temp_file('XREF', auditor)
    for line in edb.iterrows():
        temp_File.write_text_line(line[1].to_json())

    # ... and file to blob in GVA
    preFormat = BLOB_FOR_MITRE_CVE_EXPLOITS.replace('%date', '%Y-%m-%d')
    destinationFile = datetime.datetime.today().strftime(preFormat) # always write this as at today as the source data is always as at today
    temp_File.save_to_bucket(TARGET_PROJECT, TARGET_BUCKET, destinationFile)
    temp_File = None

    # ExploitDB source data to temporary file...
    temp_File = dt.temp_file('ExploitDB', auditor)
    for line in ExploitDBDataRaw.iterrows():
        temp_File.write_text_line(line[1].to_json())
        
    # ... and again to blob in GVA
    preFormat = BLOB_FOR_EXPLOITDB.replace('%date', '%Y-%m-%d')
    destinationFile = datetime.datetime.today().strftime(preFormat) # always write this as at today as the source data is always as at today
    temp_File.save_to_bucket(TARGET_PROJECT, TARGET_BUCKET, destinationFile)
    temp_File = None

    # Tidy up auditor
    auditor.completion_time = datetime.datetime.today()
    auditor.log_event()
    
main()
